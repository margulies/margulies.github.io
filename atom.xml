<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[satrajit.ghosh]]></title>
  <link href="http://satra.cogitatum.org/atom.xml" rel="self"/>
  <link href="http://satra.cogitatum.org/"/>
  <updated>2013-03-16T16:14:19-04:00</updated>
  <id>http://satra.cogitatum.org/</id>
  <author>
    <name><![CDATA[Satrajit Ghosh]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Provenance and metadata]]></title>
    <link href="http://satra.cogitatum.org/blog/2013/02/20/provenance-and-metadata/"/>
    <updated>2013-02-20T10:54:00-05:00</updated>
    <id>http://satra.cogitatum.org/blog/2013/02/20/provenance-and-metadata</id>
    <content type="html"><![CDATA[<p>The standards for datasharing taskforce of the INCF was tasked with establishing
data models and standards for sharing brain imaging and associated phenotypic,
genotypic and other metadata. This post is a summary of the problem space and
some of the efforts I have been associated with in addressing these issues.</p>

<!-- more -->


<h5>the goals</h5>

<p>I think the taskforce always had in mind two goals: data sharing and standards
for data sharing. These goals are linked, but the former very much requires the
latter. There is the further dichotomy in this that we need a way to address
existing data stored in a variety of databases and new data streaming in.</p>

<h5>the obstacles</h5>

<p>The biggest problem with data sharing is the fact that we do not really have any
common standards - perhaps NIFTI-1 and MINC-2 serve as standards for brain
imaging file formats, but in my view addresses somewhat of a narrow slice.
Throughout the brain imaging world, we use terms that we have come to recognize
as meaning something, but without a formal definition even when two people use
the same term we don&#8217;t know if they meant the &#8220;same&#8221; thing. For example, does
&#8220;gradient vector&#8221; mean the same thing for every person and program dealing with
diffusion weighted imaging data?</p>

<p>And this has turned out to be the biggest roadblock on our way to setting
standards. After all, with increasing computational approaches to data sharing
and exploration, machines more than humans need to be able to process the data
and humans to be able to see slices of data in intuitive ways. Across software
packages, imaging database providers, such common terminology is missing.</p>

<p>In addition to not having common terminology, we do not have a standard
mechanism for capturing provenance of data. After all, data on their own do not
implicitly tell us about sources of variance related to the processing
components. Efforts towards this have come from the <a href="http://provenance.loni.ucla.edu/">LONI group</a>, but are
very specific to workflow components of brain imaging and have not, till date,
reconciled with efforts to standardize provenance across the web such as the
<a href="http://www.w3.org/TR/prov-dm/">W3C provenance model</a>. Provenance needs to be captured at every stage
of data generation, not just for brain image processing software.</p>

<h5>the solutions</h5>

<p><em>the terms</em></p>

<p>Towards this we have started by defining and including terms into
<a href="http://neurolex.org/wiki/Main_Page">NeuroLex</a> and working with the <a href="http://www.neuinfo.org/">NIF</a> folks to make the platform
for term curation and inclusion better. Dicom terms are already
<a href="http://neurolex.org/wiki/Category:DICOM_term">there</a>, statistic terms are being developed and other terms will
be automatically included and curated once the input interface is refined.</p>

<p><em>provenance and the domain agnostic data model</em></p>

<p>We have also developed a technology agnostic data model (not an object model) on
top of the <a href="http://www.w3.org/TR/prov-dm/">W3C-PROV model</a> called <a href="http://nidm.nidash.org/">NIDM</a> to capture not just
entities and their attributes (as is currently done in most databases) but also
how they were derived (provenance). By representing this model in <a href="http://www.w3.org/RDF/">RDF</a> for
example, one can immediately leverage federated <a href="http://www.w3.org/TR/rdf-sparql-query/">SPARQL queries</a>. Domain
specificity is included via key-value pairs and through object models
(collections). The power of this approach is that by storing data and their
provenance richly one has access to this information and therefore one can
always transform or re-purpose the content (e.g., for more efficient searches on
specific data types).</p>

<p><em>the domain specific object models</em></p>

<p>We can turn some objects into this model fairly easily. For example,
<a href="http://freesurfer.net/">FreeSurfer</a> directory structures and statistics. Similar efforts are
underway to capture info from neuropsych assessments (COINS->neurolex mapping),
SPM.mat/FEAT.fsf,  workflow/process provenance (taking into account prior
efforts - such as LONI/Kepler/Galaxy), XNAT/csv->PROV conversion and testing.</p>

<p><em>the app framework</em></p>

<p>Unfortunately a lot of the above, while absolutely necessary for establishing
standards don&#8217;t convey to the average user a sense of progress in datasharing or
the standards. The app framework leverages the above data model to create apps
for data filtering/processing/visualization. Our initial apps will be built as
IPython notebooks as they demonstrate not just the virtues of this approach, but
are themselves executable documents that people can share and reuse.</p>

<h5>the summary</h5>

<p>We have finally gotten to the stage in this neuroinformatics world where a
number of the software we use are fairly stable and the data and object models
are getting clearer in our heads.</p>

<p>What we are hoping is that a common vocabulary + an extensible format agnostic
data representation allows us to communicate without knowing whether you do rdf,
python, java, xml, javascript or other technologies. What we do need from the
community is to pitch in towards the curation, app building and testing efforts.</p>

<p>And we still have an important job of communicating it better to others.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Help develop a winning strategy]]></title>
    <link href="http://satra.cogitatum.org/blog/2013/01/22/collaborativeproblemsolving/"/>
    <updated>2013-01-22T14:16:00-05:00</updated>
    <id>http://satra.cogitatum.org/blog/2013/01/22/collaborativeproblemsolving</id>
    <content type="html"><![CDATA[<p>A friend, Pablo Suarez, is running a competition to determine the best strategy
for winning in game called <a href="http://www.climatecentre.org/site/paying-for-predictions">&#8220;Pay for predictions&#8221;</a>. Our goal is to openly
and collaboratively help him, by providing some winning strategies for the game.
So spread the word!</p>

<!-- more -->


<p>The game intends to capture real life situations faced by the Red Cross Red
Crescent when dealing with natural disasters (e.g., flooding, earthquakes,
etc.,.). In keeping with the theme of collaborative problem solving as discussed
in the book <a href="http://www.amazon.com/Reinventing-Discovery-The-Networked-Science/dp/0691148902/ref=tmm_hrd_title_0">&#8220;Reinventing discovery&#8221; by Michael Nielsen</a> and the approach
taken to use <a href="http://www.gamesforchange.org/">games for social change</a>, the idea behind this shout-out is
to contribute solutions to the challenge.</p>

<p>Towards this, I have setup a <a href="https://github.com/satra/payforpredictions">Github repo</a> where everybody is welcome to
contribute. Do read the <a href="http://www.climatecentre.org/site/paying-for-predictions">guidelines</a> of the competition. It is possible
that there is a very simple solution, or one where a simulation can reveal the
optimal solution or many or none. The goal here is to demonstrate, much like the
<a href="http://polymathprojects.org/">&#8220;polymath project&#8221;</a>, that we can solve something quicker and possibly
better collaboratively. We understand that everybody may not have the time or
interest, but if you know someone who does, please let that person know.</p>

<p>Further, in keeping with the opensource spirit of scientific computation, I have
setup an <a href="http://nbviewer.ipython.org/">IPython notebook</a> to simulate the game. Based on the wonderful
work of the IPython team and contributors, this notebook can also deliver a
slideshow of the evolving solution. If you like Python, please contribute to the
notebook.</p>

<p><a href="http://nbviewer.ipython.org/urls/raw.github.com/satra/payforpredictions/master/payforpredictions.ipynb">Notebook</a></p>

<p><a href="http://slideviewer.herokuapp.com/url/raw.github.com/satra/payforpredictions/master/payforpredictions.ipynb#/">Slideshow</a></p>

<p>If you prefer something else, please add your code/document to your fork and
send a pull-request. Let us use the Github <a href="https://help.github.com/articles/using-pull-requests">pull-request system</a>,
<a href="https://github.com/satra/payforpredictions/issues/">issue tracker</a> and <a href="https://github.com/satra/payforpredictions/wiki/">wiki</a> for discussions.</p>

<p>So come one and come all and help develop a winning strategy. Go
<a href="https://github.com/satra/payforpredictions">fork and contribute</a>!</p>
]]></content>
  </entry>
  
</feed>
